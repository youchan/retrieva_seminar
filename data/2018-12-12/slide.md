# 黒猫先生と学ぶWord2Vec超入門

レトリバセミナー 2018/12/12 大崎 瑶

![shinra](shinra-chan.png)

## whoami

* 名前
  - 大崎 瑶(よう)
* 略歴
  - 1998 - 2000 筑波大学 大学院 電子情報工学研究科 (工学修士了)
  - 2009 - 2014 株式会社アプレッソ
  - 2014 - 2017 株式会社ユビレジ
  - 2017/11 株式会社レトリバ 入社
* 趣味
  - お酒 🍶
  - 電子工作
  - プログラミング
  - 技術書の同人誌

![books](books.png)

## 趣味のプログラミング

* Rubyで出来ることを増やす活動
    - Opal(A comnpiler from  Ruby to JavaScript)
        - ブラウザで動くRuby
    - Red Data Tools 
        - Rubyでデータ処理
        - Rubyで自然言語処理

## 技術書の同人誌

* 「Pragmatic Opal」
* 「ISeq探訪」
* 「猫と森羅と日本語とRuby」

## 猫と森羅と日本語とRuby

![portlait-left](charactor.png)

%right:

* 猫: サークル名、飼い猫、自分自身
* 森羅: 森羅プロジェクト
* 日本語: 日本語自然言語処理
* Ruby: Rubyで自然言語処理、Red Chainer

## 森羅プロジェクト

![middle](shinra.png)

## Red Chainer

* 深層学習フレームワーク
* Chainer(Python)のRuby実装
* CumoというGPUライブラリでGPU対応

## Red Chainerで森羅プロジェクトに挑戦

* 最後まで解くことができませんでした。
* Red Chainerにディープラーニングのアルゴリズムを実装
    - Word2Vec
    - LSTM

## Word2Vec

%large: 「王様」 - 「男」 + 「女」 = 「女王」

## Word2Vecのまえに自然言語処理一般の話

誤解を恐れずに言いきると、

%large: 自然言語処理の多くは単語とその並び(=文章)の統計的な特徴を見つけだしてそれを応用する技術である。

---

さらに誤解を恐れずに言うと、

%large: 深層学習は考える機械ではなく、多くの事例から見つけてきたパターンを新しい事柄に対して適用できるようにしたもの

---

![portlait notitle](ai_vs.jpg)

## Word2Vec

%large: Word →  Vec(tor)

%large: (単語 →  ベクトル)

## 「王様」 - 「男」 + 「女」 = 「女王」

![middle](king_queen.png)

---

%large: ではWord(単語)とは何なのか？

---

%large: ここで言う単語とは言語学的な単語ではない

---

%large: それは統計的な量として表わす必要がある

## 単語の表現

* 単語ID
* one-hotベクトル

## 単語が4つしかない世界を想像してください

%large: アデニン(A) グアニン(G) シトシン(C) チミン(T)

知っているすべての単語のことをボキャブラリといいます

## 単語ID

![middle](ids.png)

## one-hotベクトル

![large](one-hot.png)

## Bag of Words (BoW)

![large](bow.png)

## 自然言語の世界

* ボキャブラリはもっともっと大きい(何万単語になることも)
* BoWは文章の単語数とかになる

![middle](large-one-hot.png)

## one-hotベクトルとは何なのか？

単語と単語の関係は解っていない

![middle](li-indep.png)

## 線形独立

![large](li-indep2.png)

## 単語埋め込みベクトル

単語同士の関係をベクトルに埋め込む

![middle](king_queen.png)

## Word2Vec

単語ID →  one-hotベクトル

単語同士の関係は独立

one-hotベクトル -> 単語埋め込みベクトル

単語同士の関係を表現

---

%large: 単語埋め込みベクトルはどうやって作る？

---

%large: 突然ですが、先日TOEICの試験を受けてきました。

## 穴埋め問題


`Sean still remembers the first day [　　　] he entered the company building, excited about his future.`

1. where
2. why
3. when

## 穴埋め問題をどう解くか？

%large: 周辺の単語を見て決めます。

## 例えば女王なら

* ロイヤルファミリー(特に家長)のことについて話しているときには、王様や女王が出てきそうです。
* あるいはドレスやメイクといった女性特有の話題であれば、女王である確立は高まります。

![w30](king_queen2.png)

## 穴埋め問題をどうやって作るか

%large: たくさんの文章を集めてきて、あらゆる箇所に穴を開ければよいのです。

---

#### 舞踏会の前に女王は家来に、こんなドレスじゃ踊れないわ！と言いました。

---

[　　　]の前に女王は家来に、こんなドレスじゃ踊れないわ！と言いました。

舞踏会[　]前に女王は家来に、こんなドレスじゃ踊れないわ！と言いました。

舞踏会の[　]に女王は家来に、こんなドレスじゃ踊れないわ！と言いました。

舞踏会の前[　]女王は家来に、こんなドレスじゃ踊れないわ！と言いました。

舞踏会の前に[　　　]は家来に、こんなドレスじゃ踊れないわ！と言いました。

## ニューラルネットワークで解く

![large](neuralnet.png)

## word2vec

![middle](word2vec.png)

## まとめ

* 自然言語処理では単語同士の関係は始めから与えられているわけではない。そのため、one-hotベクトルという表現をもちいることが多い。
* 単語同士の関係を表わすために単語埋め込みベクトルというものをつくる。その手法としてWord2Vecがある。
* Word2Vecはたくさんの文章を集めてきて穴埋め問題としてたくさん解かせた学習の結果である。

---

このように

%large: 自然言語処理の多くは単語とその並び(=文章)の統計的な特徴を見つけだしてそれを応用する技術である。

---
